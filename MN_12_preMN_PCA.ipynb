{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import connectome_create\n",
    "# viz_method = one of ['itkwidgets', 'vtk']\n",
    "viz_method = 'vtk'\n",
    "\n",
    "# import some of our favorite packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import nglui.statebuilder as ngstbld\n",
    "\n",
    "# # this is the EM specific package for querying the EM data\n",
    "from caveclient import CAVEclient\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# from meshparty import trimesh_io, trimesh_vtk\n",
    "# from meshparty import skeletonize, skeleton_io, skeleton\n",
    "# import cloudvolume\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import json\n",
    "\n",
    "# with open(Path.home() / '.cloudvolume/secrets/'/'cave-secret.json') as f:\n",
    "#         tokens = json.load(f)\n",
    "        \n",
    "# seg_source = 'graphene://https://cave.fanc-fly.com/segmentation/table/mar2021_prod'\n",
    "# cv = cloudvolume.CloudVolume(cloudpath=seg_source, use_https=True, secrets=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datastack_name = 'fanc_production_mar2021'\n",
    "\n",
    "client = CAVEclient(datastack_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client = CAVEclient()\n",
    "\n",
    "# # if not os.path.isfile(os.path.expanduser(\"~/.cloudvolume/secrets/cave-secret.json\")):\n",
    "# client.auth.get_new_token(open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not yet setup this computer, uncomment this below line\n",
    "# paste the token from the website in, and run the line\n",
    "\n",
    "# client.auth.save_token(token=\"c14cd7a3e18a1a697716a399afbf5778\", overwrite=True)\n",
    "\n",
    "# then comment or delete the line as you don't need to run it on this computer  again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Motor neuron table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_to_mn_df = connectome_create.load_pre_to_mn_df(ext='matched_typed_with_nt')\n",
    "pre_to_mn_df.shape\n",
    "pre_to_mn_df.sum(axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpool_dict = utils.get_motor_pool_tuple_dict()\n",
    "pool_keys = [\n",
    "    'thorax_swing',\n",
    "    'thorax_stance',\n",
    "    'trochanter_extension',\n",
    "    'trochanter_flexion',\n",
    "    'femur_reductor',\n",
    "    'tibia_extensor',\n",
    "    'main_tibia_flexor',      # main_tibia_flexor for both main and A groups, or main_tibia_flexor_muscle for the muscle\n",
    "    'auxiliary_tibia_flexor_A',\n",
    "    'auxiliary_tibia_flexor_B',\n",
    "    'auxiliary_tibia_flexor_E',\n",
    "    'ltm',\n",
    "    'tarsus_depressor_med_venU',\n",
    "    'tarsus_depressor_noid',\n",
    "    ]\n",
    "pre_to_mn_df.head()\n",
    "\n",
    "new_idx = utils.sort_segment_fcn_index(pre_to_mn_df.columns)\n",
    "pre_to_mn_df = pre_to_mn_df[new_idx]\n",
    "\n",
    "pre_to_mn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just pool connectivity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flexors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All = slice(None)\n",
    "femur_df_local = pre_to_mn_df.loc[('local',All,All,All),('L','Leg','femur',All,All,All,All)]\n",
    "# femur_df_all = pre_to_mn_df.loc[('local'),('L','Leg','femur',All,All,All,All)]\n",
    "\n",
    "# mft_df = femur_df_local.loc[('main_tibia_flexor',All,All),mpool_dict['main_tibia_flexor_muscle']]\n",
    "mft_df = femur_df_local.loc[('local','main_tibia_flexor',All,All),mpool_dict['aux_main_tibia_flexor']]\n",
    "# mft_df = mft_df.iloc[0:50,:]\n",
    "\n",
    "# just change the rank label on that one MN\n",
    "clms = mft_df.columns.to_frame()\n",
    "clms.loc[('L','Leg','femur','flex','main_tibia_flexor','0')].loc[[False,True],'rank']='1'\n",
    "# clms.loc[('L','Leg','femur','flex','main_tibia_flexor','0',All),:].loc[[False,True],'rank']\n",
    "mft_df.columns = pd.MultiIndex.from_frame(clms)\n",
    "mft_df\n",
    "\n",
    "ext_df = femur_df_local.loc[('local','tibia_extensor',All,All),mpool_dict['tibia_extensor']]\n",
    "ext_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leg modules: flexors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(mft_df.to_numpy())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "pca_decomp = PCA(n_components=None,\n",
    "          copy=True, \n",
    "          whiten=False, \n",
    "          svd_solver='full', \n",
    "          tol=0.0, \n",
    "          iterated_power='auto')\n",
    "mft_pca = pca_decomp.fit(mft_df)\n",
    "mu = np.mean(mft_df, axis=0)\n",
    "mu\n",
    "\n",
    "# nComp = 5\n",
    "# Xhat = np.dot(mft_pca.transform(mft_df)[:,:nComp], mft_pca.components_[:nComp,:])\n",
    "# Xhat += mu\n",
    "\n",
    "A0 = mft_pca.transform(mft_df)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(A0)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'projection',)\n",
    "plt.show()\n",
    "\n",
    "# fig = plt.figure(1, figsize = [6,6])\n",
    "# plt.plot(A0[0,:])\n",
    "# plt.plot(A0[1,:])\n",
    "# plt.plot(A0[2,:])\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(mft_pca.explained_variance_/mft_pca.explained_variance_.sum())\n",
    "A0var = np.var(A0,axis=0)\n",
    "A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "plt.ylabel('% variance')\n",
    "plt.xlabel('PC')\n",
    "plt.title('Tibia Flexors (9 MNs)')\n",
    "\n",
    "mft_pca.components_.shape\n",
    "V = mft_pca.components_\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = [6,6])\n",
    "ax = sns.heatmap(V.transpose())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "(V[:,0]**2).sum()\n",
    "# So, the test is whether AV gives back the original matrix\n",
    "Y = np.matmul(A0,V)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "Y += mu.to_numpy()\n",
    "ax = sns.heatmap(Y)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "mft_pca.explained_variance_/mft_pca.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(V[0,:])\n",
    "plt.plot(V[1,:])\n",
    "plt.plot(V[2,:])\n",
    "\n",
    "plt.plot(mu.to_numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add preMNs together and project onto components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of N=1000 pairs\n",
    "from numpy.random import default_rng\n",
    "\n",
    "N = 1000\n",
    "k = 2\n",
    "\n",
    "rng = default_rng()\n",
    "n = mft_df.shape[0]\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "\n",
    "sumN = 15 # number of rows to sum\n",
    "A_var_mat = np.full((sumN-1,len(A0var)),fill_value=0,dtype=float)\n",
    "A_var_mat[0,:] = A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "\n",
    "q = 0\n",
    "for k in range(sumN-1)[1:]:\n",
    "    rnd_rows = rng.integers(0,n-1,(N,k+1)) # start with 2 and go to sumN\n",
    "\n",
    "    n_k_mat = np.full((N,mft_df.shape[1]),fill_value=0,dtype=float)\n",
    "    for r in range(n_k_mat.shape[0]):\n",
    "        n_k_mat[r,:] = mft_df.iloc[rnd_rows[r,:],:].sum(axis=0)\n",
    "\n",
    "    A = mft_pca.transform(n_k_mat)\n",
    "    \n",
    "    A_var = np.var(A,axis=0)\n",
    "\n",
    "    plt.plot(A_var/A_var.sum())\n",
    "    A_var_mat[k,:] = A_var\n",
    "    q = q+1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: no change\n",
    "The original pcs continue to capture variance as more preMNs are added together. That's likely because the 2nd 3rd etc pcs are capturing variation along a different axis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA sums of ~35 neurons\n",
    "Try PCA on a matrix of simulated sums. Get the major components, then project individuals and sums onto those pcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of N=1000 pairs\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "N = 2500\n",
    "n = mft_df.shape[0]\n",
    "k = np.floor(N/2).astype(int)\n",
    "rnd_rows = rng.integers(0,n-1,(N,k+1))\n",
    "n_k_mat = np.full((N,mft_df.shape[1]),fill_value=0,dtype=float)\n",
    "for r in range(n_k_mat.shape[0]):\n",
    "    n_k_mat[r,:] = mft_df.iloc[rnd_rows[r,:],:].sum(axis=0)\n",
    "\n",
    "pca_decomp = PCA(n_components=None,\n",
    "          copy=True, \n",
    "          whiten=False, \n",
    "          svd_solver='full', \n",
    "          tol=0.0, \n",
    "          iterated_power='auto')\n",
    "n_k_pca = pca_decomp.fit(n_k_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = n_k_pca.components_\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(V.transpose())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to project into the n choose k space. The other components should capture a fair bit of the noise\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "\n",
    "sumN = 35 # number of rows to sum\n",
    "A_var_mat = np.full((sumN-1,len(A0var)),fill_value=0,dtype=float)\n",
    "A_var_mat[0,:] = A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "\n",
    "for k in range(sumN-1)[1:]:\n",
    "    rnd_rows = rng.integers(0,n-1,(N,k+1)) # start with 2 and go to sumN\n",
    "\n",
    "    n_k_mat = np.full((N,mft_df.shape[1]),fill_value=0,dtype=float)\n",
    "    for r in range(n_k_mat.shape[0]):\n",
    "        n_k_mat[r,:] = mft_df.iloc[rnd_rows[r,:],:].sum(axis=0)\n",
    "\n",
    "    A = n_k_pca.transform(n_k_mat)\n",
    "    \n",
    "    A_var = np.var(A,axis=0)\n",
    "\n",
    "    plt.plot(A_var/A_var.sum())\n",
    "    A_var_mat[k,:] = A_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(A_var_mat[:,0]/A_var_mat.sum(axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: No more variance onto first component!\n",
    "Huh! The variance captured by the first components is similar to when using the original pca space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leg modules: Extensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(ext_df.to_numpy())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "pca_decomp = PCA(n_components=None,\n",
    "          copy=True, \n",
    "          whiten=False, \n",
    "          svd_solver='full', \n",
    "          tol=0.0, \n",
    "          iterated_power='auto')\n",
    "ext_pca = pca_decomp.fit(ext_df)\n",
    "mu = np.mean(ext_df, axis=0)\n",
    "mu\n",
    "\n",
    "# nComp = 5\n",
    "# Xhat = np.dot(mft_pca.transform(mft_df)[:,:nComp], mft_pca.components_[:nComp,:])\n",
    "# Xhat += mu\n",
    "\n",
    "A0 = ext_pca.transform(ext_df)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(A0)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'projection',)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(ext_pca.explained_variance_/ext_pca.explained_variance_.sum())\n",
    "A0var = np.var(A0,axis=0)\n",
    "A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "plt.ylabel('% variance')\n",
    "plt.xlabel('PC')\n",
    "plt.title('Tibia Extensors (FETi, SETi)')\n",
    "\n",
    "\n",
    "ext_pca.components_.shape\n",
    "V = ext_pca.components_\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = [6,6])\n",
    "ax = sns.heatmap(V.transpose())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "(V[:,0]**2).sum()\n",
    "# So, the test is whether AV gives back the original matrix\n",
    "Y = np.matmul(A0,V)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "Y += mu.to_numpy()\n",
    "ax = sns.heatmap(Y)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_pca.explained_variance_/ext_pca.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add preMNs together and project onto components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of N=1000 pairs\n",
    "from numpy.random import default_rng\n",
    "\n",
    "N = 2500\n",
    "k = 2\n",
    "\n",
    "rng = default_rng()\n",
    "n = ext_df.shape[0]\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "\n",
    "sumN = 15 # number of rows to sum\n",
    "A_var_mat = np.full((sumN-1,len(A0var)),fill_value=0,dtype=float)\n",
    "A_var_mat[0,:] = A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "\n",
    "for k in range(sumN-1)[1:]:\n",
    "    rnd_rows = rng.integers(0,n-1,(N,k+1)) # start with 2 and go to sumN\n",
    "\n",
    "    n_k_mat = np.full((N,ext_df.shape[1]),fill_value=0,dtype=float)\n",
    "    for r in range(n_k_mat.shape[0]):\n",
    "        n_k_mat[r,:] = ext_df.iloc[rnd_rows[r,:],:].sum(axis=0)\n",
    "\n",
    "    A = ext_pca.transform(n_k_mat)\n",
    "    \n",
    "    A_var = np.var(A,axis=0)\n",
    "\n",
    "    plt.plot(A_var/A_var.sum())\n",
    "    A_var_mat[k,:] = A_var"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All leg modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "mpool_dict = utils.get_motor_pool_tuple_dict()\n",
    "pool_keys = [\n",
    "    'thorax_swing',\n",
    "    'thorax_stance',\n",
    "    'trochanter_extension', \n",
    "    'tergotrochanter',          \n",
    "    'extracoxal_trochanter_depressor',\n",
    "    'trochanter_extensor',\n",
    "    'trochanter_flexion',\n",
    "    'femur_reductor',\n",
    "    'tibia_extensor',\n",
    "    'main_tibia_flexor_wtarsus',\n",
    "    'auxiliary_tibia_flexor_B_wtarsus',\n",
    "    'auxiliary_tibia_flexor_E_wtarsus',\n",
    "    # 'main_tibia_flexor',      # main_tibia_flexor for both main and A groups, or main_tibia_flexor_muscle for the muscle\n",
    "    # 'auxiliary_tibia_flexor_B',\n",
    "    # 'auxiliary_tibia_flexor_E',\n",
    "    'ltm',\n",
    "    'ltm_tiny_small',\n",
    "    'ltm_tibia',\n",
    "    'ltm_femur',\n",
    "    # 'tarsus_depressor_med_venU',\n",
    "    # 'tarsus_depressor_noid',\n",
    "    ]\n",
    "\n",
    "pref_pool_dict = {\n",
    "    'thorax_swing': 'thorax_swing',\n",
    "    'thorax_stance': 'thorax_stance',\n",
    "    'trochanter_extension': 'trochanter_extension',\n",
    "    'tergotrochanter': 'trochanter_extension',\n",
    "    'extracoxal_trochanter_depressor' : 'trochanter_extension',\n",
    "    'trochanter_extensor': 'trochanter_extension',\n",
    "    'trochanter_flexion': 'trochanter_flexion',\n",
    "    'femur_reductor':  'femur_reductor',\n",
    "    'tibia_extensor': 'tibia_extensor',\n",
    "    'main_tibia_flexor_wtarsus': 'main_tibia_flexor',\n",
    "    'auxiliary_tibia_flexor_B_wtarsus': 'auxiliary_tibia_flexor_B',\n",
    "    'auxiliary_tibia_flexor_E_wtarsus':  'auxiliary_tibia_flexor_E',\n",
    "    'ltm': 'ltm',\n",
    "    'ltm_tiny_small': 'ltm',\n",
    "    'ltm_tibia': 'ltm',\n",
    "    'ltm_femur': 'ltm',\n",
    "    #'tarsus_depressor_med_venU': ,\n",
    "}\n",
    "fname = './figpanels/PCA_leg_modules_pct_tr_ext_ltm_breakout.svg'\n",
    "# pre_to_mn_df.head()\n",
    "# mpool_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "mpool_dict = utils.get_motor_pool_tuple_dict()\n",
    "pool_keys = [\n",
    "    'thorax_swing',\n",
    "    'thorax_stance',\n",
    "    'trochanter_extension', \n",
    "    # 'tergotrochanter',          \n",
    "    # 'extracoxal_trochanter_depressor',\n",
    "    # 'trochanter_extensor',\n",
    "    'trochanter_flexion',\n",
    "    'femur_reductor',\n",
    "    'tibia_extensor',\n",
    "    'main_tibia_flexor_wtarsus',\n",
    "    'auxiliary_tibia_flexor_B_wtarsus',\n",
    "    'auxiliary_tibia_flexor_E_wtarsus',\n",
    "    # 'main_tibia_flexor',      # main_tibia_flexor for both main and A groups, or main_tibia_flexor_muscle for the muscle\n",
    "    # 'auxiliary_tibia_flexor_B',\n",
    "    # 'auxiliary_tibia_flexor_E',\n",
    "    'ltm',\n",
    "    #'ltm_tiny_small',\n",
    "    #'ltm_tibia',\n",
    "    #'ltm_femur',\n",
    "    # 'tarsus_depressor_med_venU',\n",
    "    # 'tarsus_depressor_noid',\n",
    "    ]\n",
    "\n",
    "pref_pool_dict = {\n",
    "    'thorax_swing': 'thorax_swing',\n",
    "    'thorax_stance': 'thorax_stance',\n",
    "    'trochanter_extension': 'trochanter_extension',\n",
    "    'tergotrochanter': 'trochanter_extension',\n",
    "    'extracoxal_trochanter_depressor' : 'trochanter_extension',\n",
    "    'trochanter_extensor': 'trochanter_extension',\n",
    "    'trochanter_flexion': 'trochanter_flexion',\n",
    "    'femur_reductor':  'femur_reductor',\n",
    "    'tibia_extensor': 'tibia_extensor',\n",
    "    'main_tibia_flexor_wtarsus': 'main_tibia_flexor',\n",
    "    'auxiliary_tibia_flexor_B_wtarsus': 'auxiliary_tibia_flexor_B',\n",
    "    'auxiliary_tibia_flexor_E_wtarsus':  'auxiliary_tibia_flexor_E',\n",
    "    'ltm': 'ltm',\n",
    "    'ltm_tiny_small': 'ltm',\n",
    "    'ltm_tibia': 'ltm',\n",
    "    'ltm_femur': 'ltm',\n",
    "    #'tarsus_depressor_med_venU': ,\n",
    "}\n",
    "fname = './figpanels/PCA_leg_modules_pct.svg'\n",
    "# pre_to_mn_df.head()\n",
    "# mpool_dict\n",
    "\n",
    "var_frac_dict = {}\n",
    "pca_decomp = PCA(n_components=None,\n",
    "        copy=True, \n",
    "        whiten=False, \n",
    "        svd_solver='full', \n",
    "        tol=0.0, \n",
    "        iterated_power='auto')\n",
    "\n",
    "for pool in pool_keys:\n",
    "    syn = local_df.loc[pref_pool_dict[pool],mpool_dict[pool]]\n",
    "    print(syn.shape)\n",
    "    mu = np.mean(syn, axis=0)\n",
    "\n",
    "    syn_pca = pca_decomp.fit(syn)\n",
    "    A0 = syn_pca.transform(syn)\n",
    "    var_frac_dict[pool] = syn_pca.explained_variance_/syn_pca.explained_variance_.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df = pre_to_mn_df.loc['local',:]\n",
    "# local_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_frac_dict = {}\n",
    "pca_decomp = PCA(n_components=None,\n",
    "        copy=True, \n",
    "        whiten=False, \n",
    "        svd_solver='full', \n",
    "        tol=0.0, \n",
    "        iterated_power='auto')\n",
    "\n",
    "for pool in pool_keys:\n",
    "    syn = local_df.loc[pref_pool_dict[pool],mpool_dict[pool]]\n",
    "    print(syn.shape)\n",
    "    mu = np.mean(syn, axis=0)\n",
    "\n",
    "    syn_pca = pca_decomp.fit(syn)\n",
    "    A0 = syn_pca.transform(syn)\n",
    "    var_frac_dict[pool] = syn_pca.explained_variance_/syn_pca.explained_variance_.sum()\n",
    "\n",
    "fig, ax = plt.subplots( 1, 1, figsize=(6,6))\n",
    "clrs = sns.color_palette()\n",
    "clrs = [\n",
    "    '#555555',\n",
    "    '#888888',\n",
    "    '#A0A0A0',\n",
    "    '#BFBFBF',\n",
    "    '#DFDFDF',\n",
    "    '#E0E0E0',\n",
    "    '#E7E7E7',\n",
    "    '#EBEBEB',\n",
    "    '#EFEFEF',\n",
    "    '#F2F2F2',\n",
    "    '#F4F4F4',\n",
    "    '#F8F8F8',\n",
    "]\n",
    "\n",
    "m = 0\n",
    "pc1 = []\n",
    "pc2 = []\n",
    "for pool in pool_keys:\n",
    "    pc_frac = var_frac_dict[pool]\n",
    "    bottom = 0.\n",
    "    pc1+=[pc_frac[0]]\n",
    "    pc2+=[pc_frac[1]]\n",
    "    for pc_fr,clr in zip(pc_frac,clrs):\n",
    "        p = ax.bar(m, pc_fr, 0.8, bottom=bottom,color=clr)\n",
    "        bottom += pc_fr\n",
    "    m+=1\n",
    "\n",
    "x = [i for i in range(len(pool_keys))]\n",
    "ax.plot(x,pc1,color='#000000')\n",
    "ax.plot(x,pc2,color=clrs[1])\n",
    "ax.plot(x,np.array(pc2)/np.array(pc1))\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(pool_keys)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "fig.savefig(fname,format='svg')\n",
    "fig.savefig('./figpanels/PCA_leg_modules_pct.svg',format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 PCs for Tibia flexors\n",
    "\n",
    "# mft_df = femur_df_local.loc[('local','main_tibia_flexor',All,All),mpool_dict['aux_main_tibia_flexor']]\n",
    "\n",
    "pref_pool_dict['aux_main_tibia_flexor_wtarsus'] = 'main_tibia_flexor'\n",
    "\n",
    "syn = local_df.loc[pref_pool_dict['aux_main_tibia_flexor_wtarsus'],mpool_dict['aux_main_tibia_flexor_wtarsus']]\n",
    "print(syn.shape)\n",
    "mu = np.mean(syn, axis=0)\n",
    "\n",
    "syn_pca = pca_decomp.fit(syn)\n",
    "A0 = syn_pca.transform(syn)\n",
    "\n",
    "fig, ax = plt.subplots( 1, 1, figsize=(6,6))\n",
    "ax.plot(syn_pca.components_[0,:])\n",
    "ax.plot(syn_pca.components_[1,:])\n",
    "ax.plot(syn_pca.components_[2,:])\n",
    "fig.savefig('./figpanels/tibiaflexA_wtarsus_PCs.svg',format='svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_to_mn_df.loc[:,mpool_dict['tergotrochanter']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering synergies: Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/wingMN_adj_0314.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = df.columns.get_level_values('MN_module').unique()\n",
    "preferred_module = df.index.get_level_values('preferred_module').unique()\n",
    "preferred_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = ['DLM', 'DVM', 'ten', 'inc', 'dec', 'dow', 'unk']#,'hg2', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_dict = {\n",
    "    'DLM':'DLM',\n",
    "    'DVM':'DVM',\n",
    "    'ten':'tension',\n",
    "    'inc':'syn4',\n",
    "    'dec':'syn3',\n",
    "    'dow':'syn1',\n",
    "    'unk':'syn2',\n",
    "    'hg2':'syn5',    \n",
    "}\n",
    "var_frac_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'ten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All = slice(None)\n",
    "var_frac_dict = {}\n",
    "pca_decomp = PCA(n_components=None,\n",
    "        copy=True, \n",
    "        whiten=False, \n",
    "        svd_solver='full', \n",
    "        tol=0.0, \n",
    "        iterated_power='auto')\n",
    "\n",
    "for module in modules:\n",
    "    syn = df.loc[(All,module_dict[module],All,'local'),module]\n",
    "    print(syn.shape)\n",
    "    mu = np.mean(syn, axis=0)\n",
    "\n",
    "    syn_pca = pca_decomp.fit(syn)\n",
    "    A0 = syn_pca.transform(syn)\n",
    "    var_frac_dict[module] = syn_pca.explained_variance_/syn_pca.explained_variance_.sum()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( 1, 1, figsize=(6,6))\n",
    "clrs = [\n",
    "    '#555555',\n",
    "    '#888888',\n",
    "    '#A0A0A0',\n",
    "    '#BFBFBF',\n",
    "    '#DFDFDF',\n",
    "    '#E0E0E0',\n",
    "    '#E7E7E7',\n",
    "    '#EBEBEB',\n",
    "    '#EFEFEF',\n",
    "    '#F2F2F2',\n",
    "    '#F4F4F4',\n",
    "    '#F8F8F8',\n",
    "]\n",
    "m = 0\n",
    "pc1 = []\n",
    "pc2 = []\n",
    "for module in modules:\n",
    "    pc_frac = var_frac_dict[module]\n",
    "    pc1+=[pc_frac[0]]\n",
    "    try:\n",
    "        pc2+=[pc_frac[1]]\n",
    "    except IndexError:\n",
    "        pc2+=[0]\n",
    "    bottom = 0.\n",
    "    for pc_fr,clr in zip(pc_frac,clrs):\n",
    "        p = ax.bar(m, pc_fr, 0.8, bottom=bottom,color=clr)\n",
    "        bottom += pc_fr\n",
    "    m+=1\n",
    "\n",
    "x = [i for i in range(len(modules))]\n",
    "ax.plot(x,pc1)\n",
    "ax.plot(x,np.array(pc2)/np.array(pc1))\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(modules)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "fig.savefig('./figpanels/PCA_wing_modules_pct.svg',format='svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[]\n",
    "# module_dict['inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 3 PCs for Tibia flexors\n",
    "\n",
    "# mft_df = femur_df_local.loc[('local','main_tibia_flexor',All,All),mpool_dict['aux_main_tibia_flexor']]\n",
    "\n",
    "# pref_pool_dict['inc'] = 'main_tibia_flexor'\n",
    "syn = df.loc[(All,module_dict['inc'],All,'local'),'inc']\n",
    "# syn = df.loc[module_dict['inc'],'inc']\n",
    "print(syn.shape)\n",
    "\n",
    "mu = np.mean(syn, axis=0)\n",
    "\n",
    "syn_pca = pca_decomp.fit(syn)\n",
    "A0 = syn_pca.transform(syn)\n",
    "\n",
    "fig, ax = plt.subplots( 1, 1, figsize=(6,6))\n",
    "ax.plot(syn_pca.components_[0,:])\n",
    "ax.plot(syn_pca.components_[1,:])\n",
    "ax.plot(syn_pca.components_[2,:])\n",
    "fig.savefig('./figpanels/WBA_inc_PCs.svg',format='svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu = np.mean(syn, axis=0)\n",
    "\n",
    "syn_pca = pca_decomp.fit(syn)\n",
    "A0 = syn_pca.transform(syn)\n",
    "\n",
    "fig, ax = plt.subplots( 1, 1, figsize=(6,6))\n",
    "ax.plot(syn_pca.components_[0,:])\n",
    "ax.plot(syn_pca.components_[1,:])\n",
    "ax.plot(syn_pca.components_[2,:])\n",
    "fig.savefig('./figpanels/tibiaflexA_wtarsus_PCs.svg',format='svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(syn4_pca.explained_variance_/syn4_pca.explained_variance_.sum())\n",
    "A0var = np.var(A0,axis=0)\n",
    "A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "plt.ylabel('% variance')\n",
    "plt.xlabel('PC')\n",
    "plt.title('Syn 4 (tpn,iii1,iii3)')\n",
    "\n",
    "print(syn4_pca.components_.shape)\n",
    "V = syn4_pca.components_\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = [6,6])\n",
    "ax = sns.heatmap(V.transpose())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "(V[:,0]**2).sum()\n",
    "# So, the test is whether AV gives back the original matrix\n",
    "Y = np.matmul(A0,V)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "Y += mu.to_numpy()\n",
    "ax = sns.heatmap(Y)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "syn4_pca.explained_variance_/syn4_pca.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering synergies: syn4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(syn4.to_numpy())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "pca_decomp = PCA(n_components=None,\n",
    "          copy=True, \n",
    "          whiten=False, \n",
    "          svd_solver='full', \n",
    "          tol=0.0, \n",
    "          iterated_power='auto')\n",
    "syn4_pca = pca_decomp.fit(syn4)\n",
    "mu = np.mean(syn4, axis=0)\n",
    "mu\n",
    "\n",
    "# nComp = 5\n",
    "# Xhat = np.dot(mft_pca.transform(mft_df)[:,:nComp], mft_pca.components_[:nComp,:])\n",
    "# Xhat += mu\n",
    "\n",
    "A0 = syn4_pca.transform(syn4)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(A0)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'projection',)\n",
    "plt.show()\n",
    "\n",
    "# fig = plt.figure(1, figsize = [6,6])\n",
    "# plt.plot(A0[0,:])\n",
    "# plt.plot(A0[1,:])\n",
    "# plt.plot(A0[2,:])\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(syn4_pca.explained_variance_/syn4_pca.explained_variance_.sum())\n",
    "A0var = np.var(A0,axis=0)\n",
    "A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "plt.ylabel('% variance')\n",
    "plt.xlabel('PC')\n",
    "plt.title('Syn 4 (tpn,iii1,iii3)')\n",
    "\n",
    "print(syn4_pca.components_.shape)\n",
    "V = syn4_pca.components_\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = [6,6])\n",
    "ax = sns.heatmap(V.transpose())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "(V[:,0]**2).sum()\n",
    "# So, the test is whether AV gives back the original matrix\n",
    "Y = np.matmul(A0,V)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "Y += mu.to_numpy()\n",
    "ax = sns.heatmap(Y)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "syn4_pca.explained_variance_/syn4_pca.explained_variance_.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add preMNs together, project onto pca space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of N=1000 pairs\n",
    "from numpy.random import default_rng\n",
    "\n",
    "N = 2500\n",
    "k = 2\n",
    "\n",
    "rng = default_rng()\n",
    "n = syn4.shape[0]\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "\n",
    "sumN = 15 # number of rows to sum\n",
    "A_var_mat = np.full((sumN-1,len(A0var)),fill_value=0,dtype=float)\n",
    "A_var_mat[0,:] = A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "\n",
    "for k in range(sumN-1)[1:]:\n",
    "    rnd_rows = rng.integers(0,n-1,(N,k+1)) # start with 2 and go to sumN\n",
    "\n",
    "    n_k_mat = np.full((N,syn4.shape[1]),fill_value=0,dtype=float)\n",
    "    for r in range(n_k_mat.shape[0]):\n",
    "        n_k_mat[r,:] = syn4.iloc[rnd_rows[r,:],:].sum(axis=0)\n",
    "\n",
    "    A = syn4_pca.transform(n_k_mat)\n",
    "    \n",
    "    A_var = np.var(A,axis=0)\n",
    "\n",
    "    plt.plot(A_var/A_var.sum())\n",
    "    A_var_mat[k,:] = A_var\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering synergies: Syn 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(syn3.to_numpy())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "pca_decomp = PCA(n_components=None,\n",
    "          copy=True, \n",
    "          whiten=False, \n",
    "          svd_solver='full', \n",
    "          tol=0.0, \n",
    "          iterated_power='auto')\n",
    "syn3_pca = pca_decomp.fit(syn3)\n",
    "mu = np.mean(syn3, axis=0)\n",
    "mu\n",
    "\n",
    "# nComp = 5\n",
    "# Xhat = np.dot(mft_pca.transform(mft_df)[:,:nComp], mft_pca.components_[:nComp,:])\n",
    "# Xhat += mu\n",
    "\n",
    "A0 = syn3_pca.transform(syn3)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(A0)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'projection',)\n",
    "plt.show()\n",
    "\n",
    "# fig = plt.figure(1, figsize = [6,6])\n",
    "# plt.plot(A0[0,:])\n",
    "# plt.plot(A0[1,:])\n",
    "# plt.plot(A0[2,:])\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(syn3_pca.explained_variance_/syn3_pca.explained_variance_.sum())\n",
    "A0var = np.var(A0,axis=0)\n",
    "A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "plt.ylabel('% variance')\n",
    "plt.xlabel('PC')\n",
    "plt.title('Syn 3 (i2_u,i1,b3_u)')\n",
    "\n",
    "print(syn3_pca.components_.shape)\n",
    "V = syn3_pca.components_\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = [6,6])\n",
    "ax = sns.heatmap(V.transpose())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "(V[:,0]**2).sum()\n",
    "# So, the test is whether AV gives back the original matrix\n",
    "Y = np.matmul(A0,V)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "Y += mu.to_numpy()\n",
    "ax = sns.heatmap(Y)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "syn3_pca.explained_variance_/syn3_pca.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add preMNs together, project onto pca space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of N=1000 pairs\n",
    "from numpy.random import default_rng\n",
    "\n",
    "N = 2500\n",
    "k = 2\n",
    "\n",
    "rng = default_rng()\n",
    "n = syn3.shape[0]\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "\n",
    "sumN = 15 # number of rows to sum\n",
    "A_var_mat = np.full((sumN-1,len(A0var)),fill_value=0,dtype=float)\n",
    "A_var_mat[0,:] = A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "\n",
    "for k in range(sumN-1)[1:]:\n",
    "    rnd_rows = rng.integers(0,n-1,(N,k+1)) # start with 2 and go to sumN\n",
    "\n",
    "    n_k_mat = np.full((N,syn3.shape[1]),fill_value=0,dtype=float)\n",
    "    for r in range(n_k_mat.shape[0]):\n",
    "        n_k_mat[r,:] = syn3.iloc[rnd_rows[r,:],:].sum(axis=0)\n",
    "\n",
    "    A = syn3_pca.transform(n_k_mat)\n",
    "    \n",
    "    A_var = np.var(A,axis=0)\n",
    "\n",
    "    plt.plot(A_var/A_var.sum())\n",
    "    A_var_mat[k,:] = A_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering synergies: Syn 1 (b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(syn1.to_numpy())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "pca_decomp = PCA(n_components=None,\n",
    "          copy=True, \n",
    "          whiten=False, \n",
    "          svd_solver='full', \n",
    "          tol=0.0, \n",
    "          iterated_power='auto')\n",
    "syn1_pca = pca_decomp.fit(syn1)\n",
    "mu = np.mean(syn1, axis=0)\n",
    "mu\n",
    "\n",
    "# nComp = 5\n",
    "# Xhat = np.dot(mft_pca.transform(mft_df)[:,:nComp], mft_pca.components_[:nComp,:])\n",
    "# Xhat += mu\n",
    "\n",
    "A0 = syn1_pca.transform(syn1)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(A0)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'projection',)\n",
    "plt.show()\n",
    "\n",
    "# fig = plt.figure(1, figsize = [6,6])\n",
    "# plt.plot(A0[0,:])\n",
    "# plt.plot(A0[1,:])\n",
    "# plt.plot(A0[2,:])\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(syn1_pca.explained_variance_/syn1_pca.explained_variance_.sum())\n",
    "A0var = np.var(A0,axis=0)\n",
    "A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "plt.ylabel('% variance')\n",
    "plt.xlabel('PC')\n",
    "plt.title('Syn 1 (b1, b2)')\n",
    "\n",
    "print(syn1_pca.components_.shape)\n",
    "V = syn1_pca.components_\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = [6,6])\n",
    "ax = sns.heatmap(V.transpose())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "(V[:,0]**2).sum()\n",
    "# So, the test is whether AV gives back the original matrix\n",
    "Y = np.matmul(A0,V)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "Y += mu.to_numpy()\n",
    "ax = sns.heatmap(Y)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "syn1_pca.explained_variance_/syn1_pca.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add preMNs together, project onto pca space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try a normalized version\n",
    "Just for kicks and understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First normalize by the sum of the rows\n",
    "ratiodf = mft_df.divide(mft_df.sum(axis=1),axis='index')\n",
    "ratiodf.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "ratiodf = ratiodf.dropna()\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(ratiodf.to_numpy())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n",
    "\n",
    "pca_decomp_n = PCA(n_components=None,\n",
    "          copy=True, \n",
    "          whiten=False, \n",
    "          svd_solver='full', \n",
    "          tol=0.0, \n",
    "          iterated_power='auto')\n",
    "mft_pca_n = pca_decomp_n.fit(ratiodf)\n",
    "mu = np.mean(ratiodf, axis=0)\n",
    "\n",
    "# nComp = 5\n",
    "# Xhat = np.dot(mft_pca.transform(mft_df)[:,:nComp], mft_pca.components_[:nComp,:])\n",
    "# Xhat += mu\n",
    "\n",
    "A0 = mft_pca_n.transform(ratiodf)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(A0)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'projection',)\n",
    "plt.show()\n",
    "\n",
    "# fig = plt.figure(1, figsize = [6,6])\n",
    "# plt.plot(A0[0,:])\n",
    "# plt.plot(A0[1,:])\n",
    "# plt.plot(A0[2,:])\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "plt.plot(mft_pca_n.explained_variance_/mft_pca_n.explained_variance_.sum())\n",
    "A0var = np.var(A0,axis=0)\n",
    "A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "\n",
    "print(mft_pca_n.components_.shape)\n",
    "V = mft_pca_n.components_\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = [6,6])\n",
    "ax = sns.heatmap(V.transpose())\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "(V[:,0]**2).sum()\n",
    "# So, the test is whether AV gives back the original matrix\n",
    "Y = np.matmul(A0,V)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "Y += mu.to_numpy()\n",
    "ax = sns.heatmap(Y)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add preMNs together, project onto pca_n space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of N=1000 pairs\n",
    "from numpy.random import default_rng\n",
    "\n",
    "N = 2500\n",
    "k = 2\n",
    "\n",
    "rng = default_rng()\n",
    "n = ratiodf.shape[0]\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "\n",
    "sumN = 15 # number of rows to sum\n",
    "A_var_mat = np.full((sumN-1,len(A0var)),fill_value=0,dtype=float)\n",
    "A_var_mat[0,:] = A0var\n",
    "plt.plot(A0var/A0var.sum())\n",
    "\n",
    "for k in range(sumN-1)[1:]:\n",
    "    rnd_rows = rng.integers(0,n-1,(N,k+1)) # start with 2 and go to sumN\n",
    "\n",
    "    n_k_mat = np.full((N,ratiodf.shape[1]),fill_value=0,dtype=float)\n",
    "    for r in range(n_k_mat.shape[0]):\n",
    "        n_k_mat[r,:] = ratiodf.iloc[rnd_rows[r,:],:].sum(axis=0)\n",
    "\n",
    "    A = mft_pca_n.transform(n_k_mat)\n",
    "    \n",
    "    A_var = np.var(A,axis=0)\n",
    "\n",
    "    plt.plot(A_var/A_var.sum())\n",
    "    A_var_mat[k,:] = A_var"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try SVD instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bar = mft_df.mean(axis=1)\n",
    "A = mft_df.subtract(mft_df.mean(axis=1),axis='index').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "\n",
    "ax = sns.heatmap(A)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'log 10 # of synapses',)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(A)\n",
    "u.shape, s.shape, vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(vh)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIG = np.diag(s)\n",
    "\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(SIG)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'dictionary transform of local_df',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_pca = np.matmul(A,vh.T)\n",
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(A_pca)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'projection',)\n",
    "plt.show()\n",
    "\n",
    "A_svh=  np.matmul(SIG,A_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize = [6,6])\n",
    "ax = sns.heatmap(A_pca)\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(label = 'projection',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_to_mn_df.columns.get_level_values('segID').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b93c3ce0f0b01938714f8d6ce3882059af39419bb08f14e121c5729b1321faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
